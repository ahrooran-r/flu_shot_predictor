{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", 200)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier, Perceptron\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "\n",
    "print(\"import success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data => rows: 53415, cols: 104\n",
      "labels => rows: 26707, cols: 2\n"
     ]
    }
   ],
   "source": [
    "# import pre-processed datasets\n",
    "\n",
    "# train data\n",
    "data = pd.read_csv(r\"../files/for_train.csv\", index_col=\"respondent_id\")\n",
    "\n",
    "# target\n",
    "labels = pd.read_csv(r\"../files/training_set_labels.csv\", index_col=\"respondent_id\")\n",
    "label_h1n1 = labels[[\"h1n1_vaccine\"]]\n",
    "\n",
    "# check whether rows are equal\n",
    "print(\"data => rows: %s, cols: %s\" % (data.shape[0], data.shape[1]))\n",
    "print(\"labels => rows: %s, cols: %s\" % (labels.shape[0], labels.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = label_h1n1.values.ravel()\n",
    "\n",
    "train = data[data.type.eq(\"train\")].drop(\"type\", axis=1)\n",
    "test = data[data.type.eq(\"test\")].drop(\"type\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train,\n",
    "    target,\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify = target,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# size = 1000\n",
    "size = len(y_train)\n",
    "X_train_pruned = X_train.head(size).copy()\n",
    "y_train_pruned = copy.deepcopy(y_train[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select models for pre-mature testing\n",
    "\n",
    "# Reference: https://towardsdatascience.com/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362\n",
    "\n",
    "models = [\n",
    "    BernoulliNB(),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    LinearSVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    XGBClassifier(),\n",
    "    XGBRFClassifier(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    CatBoostClassifier(verbose=0),\n",
    "    SVC(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SGDClassifier(),\n",
    "    Perceptron(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model: BernoulliNB()\n",
      "Completed model: GaussianNB()\n",
      "Completed model: DecisionTreeClassifier()\n",
      "Completed model: ExtraTreeClassifier()\n",
      "Completed model: ExtraTreesClassifier()\n",
      "Completed model: KNeighborsClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda\\envs\\data_science\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model: LinearSVC()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda\\envs\\data_science\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model: LogisticRegression()\n",
      "Completed model: RandomForestClassifier()\n",
      "Completed model: AdaBoostClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda\\envs\\data_science\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Completed model: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda\\envs\\data_science\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Completed model: XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=8, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact', validate_parameters=1,\n",
      "                verbosity=None)\n",
      "Completed model: HistGradientBoostingClassifier()\n",
      "Completed model: LGBMClassifier()\n",
      "Completed model: <catboost.core.CatBoostClassifier object at 0x000002DFF6C7AEC8>\n",
      "Completed model: SVC()\n",
      "Completed model: GradientBoostingClassifier()\n",
      "Completed model: SGDClassifier()\n",
      "Completed model: Perceptron()\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "<catboost.core.CatBoostClassifier object at 0x000002DFF6C7AEC8>:\t0.7324576513446753\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None):\t0.729031643296415\n",
      "HistGradientBoostingClassifier():\t0.7259685713657436\n",
      "LGBMClassifier():\t0.7251508656120647\n",
      "GradientBoostingClassifier():\t0.7154656231642458\n",
      "AdaBoostClassifier():\t0.714123408751305\n",
      "SGDClassifier():\t0.7062110663054758\n",
      "SVC():\t0.700592258130722\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=8, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact', validate_parameters=1,\n",
      "                verbosity=None):\t0.7005225191075499\n",
      "Perceptron():\t0.699123759540686\n",
      "LogisticRegression():\t0.6973833206455782\n",
      "RandomForestClassifier():\t0.6964025763647539\n",
      "LinearSVC():\t0.6926278732006337\n",
      "ExtraTreesClassifier():\t0.6917247214365819\n",
      "DecisionTreeClassifier():\t0.688407196313256\n",
      "BernoulliNB():\t0.6846388806572641\n",
      "KNeighborsClassifier():\t0.6705061524268866\n",
      "GaussianNB():\t0.6508811096253464\n",
      "ExtraTreeClassifier():\t0.6392658973035292\n"
     ]
    }
   ],
   "source": [
    "# pre-mature testing to select a good model\n",
    "def fit_predict(model):\n",
    "    model.fit(X_train_pruned, y_train_pruned)\n",
    "    predicted_vals = model.predict(X_test)\n",
    "    # score = f1_score(y_test, predicted_vals, average=\"weighted\")\n",
    "    score = roc_auc_score(y_test, predicted_vals)\n",
    "\n",
    "    return model, score\n",
    "\n",
    "result = []\n",
    "for model in models:\n",
    "    result.append(fit_predict(model))\n",
    "    print(\"Completed model: %s\" % model)\n",
    "\n",
    "# summary\n",
    "print(\"\\n--------------------------------------------\\n\")\n",
    "\n",
    "result.sort(key = lambda i: i[-1], reverse=True)\n",
    "\n",
    "for model, score in result:\n",
    "    print(\"%s:\\t%s\" %(model, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "CatBoostClassifier():\t0.7324576513446753\n",
    "XGBClassifier():\t0.729031643296415\n",
    "HistGradientBoostingClassifier():\t0.7259685713657436\n",
    "LGBMClassifier():\t0.7251508656120647\n",
    "GradientBoostingClassifier():\t0.7154656231642458\n",
    "AdaBoostClassifier():\t0.714123408751305\n",
    "SGDClassifier():\t0.7062110663054758\n",
    "SVC():\t0.700592258130722\n",
    "XGBRFClassifier():\t0.7005225191075499\n",
    "Perceptron():\t0.699123759540686\n",
    "LogisticRegression():\t0.6973833206455782\n",
    "RandomForestClassifier():\t0.6964025763647539\n",
    "LinearSVC():\t0.6926278732006337\n",
    "ExtraTreesClassifier():\t0.6917247214365819\n",
    "DecisionTreeClassifier():\t0.688407196313256\n",
    "BernoulliNB():\t0.6846388806572641\n",
    "KNeighborsClassifier():\t0.6705061524268866\n",
    "GaussianNB():\t0.6508811096253464\n",
    "ExtraTreeClassifier():\t0.6392658973035292\n",
    "\n",
    "It seems ***CatBoostClassifier*** works best overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}